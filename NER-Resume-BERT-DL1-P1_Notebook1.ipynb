{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Deep Learning 1 (AASD4010)\n",
    "### Deep Learning Project Requirements:\n",
    "\n",
    "Project: Building a AI model\n",
    "\n",
    "In this project students are expected to design, develop, train and evaluate a Deep Learning model. \n",
    "It can be one of the following models:\n",
    "1.\tA predictive model\n",
    "2.\tA classifier\n",
    "3.\tOthers? (sentiment analysis, text prediction, other…please discuss with me)\n",
    "\n",
    "Model: RNN, ANN, combination of these, NLP or use of transfer learning.\n",
    "\n",
    "Few examples: \n",
    "1)\tPredict the price for this house\n",
    "2)\tDetect fraud activity (classification)\n",
    "3)\tPredict the price of oil for the next year\n",
    "4)\tPredict revenue per day\n",
    "5)\tClassify text (NLP)\n",
    "6)\tLLMs and chatbots\n",
    "7)\tSentiment analysis\n",
    "8)\t…\n",
    "\n",
    "Database: No specific requirement on the database, of-course it should have at least 2 categories (for classification cases).  \n",
    "\n",
    "Mainly the dataset for this project can include:\n",
    "1.\tTime series (example price of a product over time)\n",
    "2.\tText (e.g. product reviews)\n",
    "\n",
    "Report:\n",
    "Put your results in a report, you are required to use the template I have provided, but of course you can add new sections. Your report needs to include the following sections (not limited to):\n",
    "•\tBackground and problem statement, \n",
    "•\tPlan of attack, what is your approach toward this?\n",
    "•\tThe Database, \n",
    "•\tThe model you picked to solve the problem, \n",
    "•\tResults, the model performance (test, valid), the loss, predictions…\n",
    "(like use of confusion matrices etc…)\n",
    "Also include the steps you took to tune your hyperparameter, architecture etc… (tell the whole story along the way)\n",
    "a.\tDid you have a benchmark model?\n",
    "b.\tDid you plan of attack made sense, now you have a better idea?\n",
    "c.\tDo you do differently next time?\n",
    "•\tConclusions\n",
    "                                                     \n",
    "In your results please comment and discuss the followings:\n",
    "1.\tBasic data analysis, e.g. data imbalance, outliers, how to deal with them?\n",
    "2.\tEvaluate the model, how?\n",
    "3.\tTuning your models, for example how your model change when the model structure, architecture, hyperparameters are changes?\n",
    "4.\tAre any of your models are overfitted? Why?\n",
    "\n",
    "What will be in your final submission:\n",
    "1.\tYour report\n",
    "•\tBackground and problem statement, \n",
    "•\tPlan of attack, what is your approach toward this problem?\n",
    "•\tThe Database (investigate the dataset, present findings), \n",
    "•\tThe model you picked to solve the problem, \n",
    "•\tResults, the model performance (test, valid), the loss, predictions…\n",
    "(like use of confusion matrices etc…)\n",
    "Also include the steps you took to tune your hyperparameter, architecture etc… (tell the whole story along the way)\n",
    "d.\tDid you plan of attack made sense, now you have a better idea?\n",
    "e.\tDo you do differently next time?\n",
    "•\tConclusions\n",
    "2.\tAll of your codes\n",
    "3.\tExample input data with model predictions on them (only few)\n",
    "\n",
    "\n",
    "Important Note:\n",
    "1.\tMake sure to document your findings and back up your conclusions using the results, use visualization (this is important). \n",
    "2.\tOnly one person from each group to submit on BB (report to include all group members),\n",
    "If D2L give you problems, due to size etc…, you can share it through an online drive like google.\n",
    "3.\tYour submission need to include: your data, code, report and your slides.\n",
    "4.\tIf we test them, your codes should run without error.\n",
    "5.\tImportant: Compress all of your files into a zip (or similar) files and only submit that single file.\n",
    "6.\tYou can submit few times, but I only consider the latest submission.\n",
    "\n",
    "\n",
    "###\n",
    "For this deep learning project, I choose the following project and I want to complete this project using a BERT model from Hugging Face to extract key information such as skills, experience, etc using NER. For library, I want to use Tensorflow, Pandas, etc.\n",
    "# Project Title: Entity Extraction Resumes\n",
    "\n",
    "\n",
    "\n",
    "### This is a small part of the dataset for this project:\n",
    "\n",
    "{\"content\": \"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\",\"annotation\":[{\"label\":[\"Skills\"],\"points\":[{\"start\":1295,\"end\":1621,\"text\":\"\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":993,\"end\":1153,\"text\":\"C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":939,\"end\":956,\"text\":\"Kendriya Vidyalaya\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":883,\"end\":904,\"text\":\"Woodbine modern school\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":856,\"end\":860,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":771,\"end\":813,\"text\":\"B.v.b college of engineering and technology\"}]},{\"label\":[\"Designation\"],\"points\":[{\"start\":727,\"end\":769,\"text\":\"B.E in Information science and engineering\\n\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":407,\"end\":415,\"text\":\"Accenture\"}]},{\"label\":[\"Designation\"],\"points\":[{\"start\":372,\"end\":404,\"text\":\"Application Development Associate\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":95,\"end\":145,\"text\":\"Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":60,\"end\":68,\"text\":\"Bengaluru\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":49,\"end\":57,\"text\":\"Accenture\"}]},{\"label\":[\"Designation\"],\"points\":[{\"start\":13,\"end\":45,\"text\":\"Application Development Associate\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":11,\"text\":\"Abhishek Jha\"}]}],\"extras\":null}\n",
    "{\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null}\n",
    "{\"content\": \"Akhil Yadav Polemaina\\nHyderabad, Telangana - Email me on Indeed: indeed.com/r/Akhil-Yadav-Polemaina/\\nf6931801c51c63b1\\n\\n● Senior System Engineer at Infosys with 3.2 years of experience in software development and\\nMaintenance.\\n● Maintained data processing using mainframe technology for multiple front end applications of\\nWalmart Retail Link platform and ensured on-time deliverables.\\n● Worked on automating the uses cases to reduce manual effort in solving repeating incidents\\nusing Service Now orchestration.\\n● Possess good analytical, logical ability and systematic approach to problem analysis, strong\\ndebugging and troubleshooting skills.\\n● Good exposure to Retail domain.\\n\\nWilling to relocate to: hyderbad, Telangana\\n\\nWORK EXPERIENCE\\n\\nSenior Systems Engineer\\n\\nInfosys Limited -  Hyderabad, Telangana -\\n\\nJanuary 2015 to Present\\n\\n● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support\\nactivities\\n● Identifying and fixing all the major defects in the applications, perform root cause analysis for\\nproduction issues\\n● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing\\nsessions with the client\\n● Leading a peer group and taking end to end responsibilities for all the critical issues/\\nenhancements.\\n● Identifying the use cases which can be automated using Service Now Orchestration\\n● Creating workflows to automate various tasks which involved manual intervention\\n● Direct interaction with the client on various business impacting issues on a daily basis\\n● Setting up Weekly Status Review meetings and Code Review meetings with the client\\n\\nSenior Systems Engineer\\n\\nInfosys Limited -  Hyderabad, Telangana -\\n\\nJanuary 2015 to Present\\n\\nTeam Size # 5\\nProject Objective:\\nProviding end to end Maintenance and Support activity for data processing of the most critical and\\nimportant Web portal 'Retail Link' along with over 40 applications used daily by all the Suppliers\\nand Business users of Walmart, the largest retailer in the world. Retail link is a portal which hosts\\n100's of applications developed across technologies for the suppliers which help them to carry\\non day-to-day activities right from on boarding to tracking their sales. This involves supporting\\n\\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nvarious Decision Support System reports which helps the higher management to take business\\ncritical decisions.\\n\\nResponsibilities:\\n● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support\\nactivities\\n● Identifying and fixing all the major defects in the applications, perform root cause analysis for\\nproduction issues\\n● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing\\nsessions with the client\\n● Leading a peer group and taking end to end responsibilities for all the critical issues/\\nenhancements.\\n● Identifying the use cases which can be automated using Service Now Orchestration\\n● Creating workflows to automate various tasks which involved manual intervention\\n● Direct interaction with the client on various business impacting issues on a daily basis\\n● Setting up Weekly Status Review meetings and Code Review meetings with the client\\n\\nEDUCATION\\n\\nElectrical and Electronics Engineering\\n\\nAnurag College of Engineering (Jntuh)\\n\\nSKILLS\\n\\nservicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), Teradata (3 years)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n• Domain - Retail\\n• Technology - Mainframe (COBOL, JCL, DB2, Teradata), Service now.\\n• Operating System - Mainframe (z/OS)\\n• Database - DB2, SQL, Teradata.\\n• Utilities - FILE-AID, IDCAMS, DFSORT basics, LIBRARIAN, FTP/SFTP, CA-7 basics.\\n• Tools - Query Management Tool (QMF), SQL Assistant, Service now, Remedy.\\n\\nKey Strengths:\\n● Effective Communication Skills and Zeal to learn.\\n● Flexibility and Adaptability.\\n● Good Leadership Qualities.\\n● Analytical and Problem Solving Skills.\\n\\nAchievements:\\n● Received STAR award for working on various system improvement and automation activities\\n● Received multiple INSTA awards for my performance in the projects worked\",\"annotation\":[{\"label\":[\"Skills\"],\"points\":[{\"start\":3749,\"end\":3756,\"text\":\"Teradata\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3709,\"end\":3717,\"text\":\"Mainframe\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3664,\"end\":3671,\"text\":\"Teradata\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3636,\"end\":3644,\"text\":\"Mainframe\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3542,\"end\":3549,\"text\":\"Teradata\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3526,\"end\":3529,\"text\":\" Jcl\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3510,\"end\":3514,\"text\":\"cobol\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3489,\"end\":3497,\"text\":\"Mainframe\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":3468,\"end\":3477,\"text\":\"servicenow\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":3421,\"end\":3457,\"text\":\"Anurag College of Engineering (Jntuh)\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":3381,\"end\":3419,\"text\":\"Electrical and Electronics Engineering\\n\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":1664,\"end\":1672,\"text\":\"Hyderabad\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":1645,\"end\":1659,\"text\":\"Infosys Limited\"}]},{\"label\":[\"Designation\"],\"points\":[{\"start\":1620,\"end\":1642,\"text\":\"Senior Systems Engineer\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":783,\"end\":791,\"text\":\"Hyderabad\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":764,\"end\":778,\"text\":\"Infosys Limited\"}]},{\"label\":[\"Designation\"],\"points\":[{\"start\":739,\"end\":761,\"text\":\"Senior Systems Engineer\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":65,\"end\":116,\"text\":\"indeed.com/r/Akhil-Yadav-Polemaina/\\nf6931801c51c63b1\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":22,\"end\":30,\"text\":\"Hyderabad\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":20,\"text\":\"Akhil Yadav Polemaina\"}]}],\"extras\":null}\n",
    "\n",
    "\n",
    "\n",
    "Give me detailed instructions to complete this project in steps such as \n",
    "1. Loading the data from the file data.json file\n",
    "2. Exploring the loaded data\n",
    "3. Pre-process the data\n",
    ".\n",
    ".\n",
    "."
   ],
   "id": "b32fe9698aa00ff4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 1: Loading the data from the data.json file",
   "id": "37518849b7c01aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:13:48.236545Z",
     "start_time": "2024-05-29T15:13:44.972453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  # JSON encoder and decoder\n",
    "import re    # Regular expression operations\n",
    "from sklearn.model_selection import train_test_split    # for splitting the data into training and validation sets\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # Padding sequences for equal length\n",
    "\n",
    "# TFBertForTokenClassification for token classification, BertConfig for model configuration\n",
    "from transformers import TFBertForTokenClassification, BertTokenizer, BertConfig"
   ],
   "id": "b85a5d786b009820",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:13:49.687926Z",
     "start_time": "2024-05-29T15:13:49.671903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the JSON data\n",
    "data = []\n",
    "with open('data.json', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Check the loaded data\n",
    "print(data[0])"
   ],
   "id": "81d9da3af923ca6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\", 'annotation': [{'label': ['Skills'], 'points': [{'start': 1295, 'end': 1621, 'text': '\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player'}]}, {'label': ['Skills'], 'points': [{'start': 993, 'end': 1153, 'text': 'C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)'}]}, {'label': ['College Name'], 'points': [{'start': 939, 'end': 956, 'text': 'Kendriya Vidyalaya'}]}, {'label': ['College Name'], 'points': [{'start': 883, 'end': 904, 'text': 'Woodbine modern school'}]}, {'label': ['Graduation Year'], 'points': [{'start': 856, 'end': 860, 'text': '2017\\n'}]}, {'label': ['College Name'], 'points': [{'start': 771, 'end': 813, 'text': 'B.v.b college of engineering and technology'}]}, {'label': ['Designation'], 'points': [{'start': 727, 'end': 769, 'text': 'B.E in Information science and engineering\\n'}]}, {'label': ['Companies worked at'], 'points': [{'start': 407, 'end': 415, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 372, 'end': 404, 'text': 'Application Development Associate'}]}, {'label': ['Email Address'], 'points': [{'start': 95, 'end': 145, 'text': 'Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n'}]}, {'label': ['Location'], 'points': [{'start': 60, 'end': 68, 'text': 'Bengaluru'}]}, {'label': ['Companies worked at'], 'points': [{'start': 49, 'end': 57, 'text': 'Accenture'}]}, {'label': ['Designation'], 'points': [{'start': 13, 'end': 45, 'text': 'Application Development Associate'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 11, 'text': 'Abhishek Jha'}]}], 'extras': None}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:13:56.040298Z",
     "start_time": "2024-05-29T15:13:56.008972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())"
   ],
   "id": "bdbbec24e831ba8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  Abhishek Jha\\nApplication Development Associat...   \n",
      "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
      "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
      "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
      "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
      "\n",
      "                                          annotation extras  \n",
      "0  [{'label': ['Skills'], 'points': [{'start': 12...   None  \n",
      "1  [{'label': ['Email Address'], 'points': [{'sta...   None  \n",
      "2  [{'label': ['Skills'], 'points': [{'start': 37...   None  \n",
      "3  [{'label': ['Skills'], 'points': [{'start': 80...   None  \n",
      "4  [{'label': ['Degree'], 'points': [{'start': 20...   None  \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: Exploring the loaded data",
   "id": "edc3302f0a144c38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:13:59.067788Z",
     "start_time": "2024-05-29T15:13:59.036569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for missing values and overall data statistics.\n",
    "print(df.info())"
   ],
   "id": "6abd57f68d40273a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 220 entries, 0 to 219\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   content     220 non-null    object\n",
      " 1   annotation  220 non-null    object\n",
      " 2   extras      0 non-null      object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:01.943310Z",
     "start_time": "2024-05-29T15:14:01.907201Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.describe())",
   "id": "29c5b83298b45f21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  content  \\\n",
      "count                                                 220   \n",
      "unique                                                219   \n",
      "top     Sameer Kujur\\nOrrisha - Email me on Indeed: in...   \n",
      "freq                                                    2   \n",
      "\n",
      "                                               annotation extras  \n",
      "count                                                 220      0  \n",
      "unique                                                220      0  \n",
      "top     [{'label': ['Skills'], 'points': [{'start': 12...    NaN  \n",
      "freq                                                    1    NaN  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:05.517348Z",
     "start_time": "2024-05-29T15:14:05.501337Z"
    }
   },
   "cell_type": "code",
   "source": "print(df.isnull().sum())",
   "id": "4f8896273dfefe10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content         0\n",
      "annotation      0\n",
      "extras        220\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3: Pre-process the data",
   "id": "2eddbf66411746cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:07.945992Z",
     "start_time": "2024-05-29T15:14:07.929801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract content and annotations\n",
    "contents = df['content']\n",
    "annotations = df['annotation']"
   ],
   "id": "234b784778bd0184",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:11.566726Z",
     "start_time": "2024-05-29T15:14:11.551172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert the annotations into a format suitable for training a Named Entity Recognition (NER) model. This includes mapping each word in the resume to its corresponding entity label.\n",
    "\n",
    "# Define a function to label the tokens\n",
    "def label_tokens(text, annotations):\n",
    "    tokens = text.split()\n",
    "    labels = ['O'] * len(tokens)  # Initialize all labels as 'O' (Outside)\n",
    "    \n",
    "    for annotation in annotations:\n",
    "        if 'label' not in annotation or not annotation['label']:\n",
    "            continue  # Skip annotations without labels\n",
    "        \n",
    "        for point in annotation['points']:\n",
    "            start, end = point['start'], point['end']\n",
    "            annotated_text = text[start:end]\n",
    "            annotated_tokens = annotated_text.split()\n",
    "            \n",
    "            if not annotated_tokens:\n",
    "                continue\n",
    "            \n",
    "            for i, token in enumerate(tokens):\n",
    "                if re.match(r'\\b' + re.escape(annotated_tokens[0]), token) and tokens[i:i+len(annotated_tokens)] == annotated_tokens:\n",
    "                    labels[i:i+len(annotated_tokens)] = ['B-' + annotation['label'][0]] + ['I-' + annotation['label'][0]] * (len(annotated_tokens) - 1)\n",
    "    \n",
    "    return tokens, labels"
   ],
   "id": "751b93f814d36dbd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:16.709728Z",
     "start_time": "2024-05-29T15:14:14.778984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text and labels\n",
    "tokenized_texts_and_labels = [\n",
    "    label_tokens(content, annotation)\n",
    "    for content, annotation in zip(contents, annotations)\n",
    "]\n",
    "\n",
    "# Extract the tokens and labels\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "# Check the tokenized texts and labels\n",
    "print(tokenized_texts[0])\n",
    "print(labels[0])"
   ],
   "id": "97c437813c32bec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhishek', 'Jha', 'Application', 'Development', 'Associate', '-', 'Accenture', 'Bengaluru,', 'Karnataka', '-', 'Email', 'me', 'on', 'Indeed:', 'indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a', '•', 'To', 'work', 'for', 'an', 'organization', 'which', 'provides', 'me', 'the', 'opportunity', 'to', 'improve', 'my', 'skills', 'and', 'knowledge', 'for', 'my', 'individual', 'and', \"company's\", 'growth', 'in', 'best', 'possible', 'ways.', 'Willing', 'to', 'relocate', 'to:', 'Bangalore,', 'Karnataka', 'WORK', 'EXPERIENCE', 'Application', 'Development', 'Associate', 'Accenture', '-', 'November', '2017', 'to', 'Present', 'Role:', 'Currently', 'working', 'on', 'Chat-bot.', 'Developing', 'Backend', 'Oracle', 'PeopleSoft', 'Queries', 'for', 'the', 'Bot', 'which', 'will', 'be', 'triggered', 'based', 'on', 'given', 'input.', 'Also,', 'Training', 'the', 'bot', 'for', 'different', 'possible', 'utterances', '(Both', 'positive', 'and', 'negative),', 'which', 'will', 'be', 'given', 'as', 'input', 'by', 'the', 'user.', 'EDUCATION', 'B.E', 'in', 'Information', 'science', 'and', 'engineering', 'B.v.b', 'college', 'of', 'engineering', 'and', 'technology', '-', 'Hubli,', 'Karnataka', 'August', '2013', 'to', 'June', '2017', '12th', 'in', 'Mathematics', 'Woodbine', 'modern', 'school', 'April', '2011', 'to', 'March', '2013', '10th', 'Kendriya', 'Vidyalaya', 'April', '2001', 'to', 'March', '2011', 'SKILLS', 'C', '(Less', 'than', '1', 'year),', 'Database', '(Less', 'than', '1', 'year),', 'Database', 'Management', '(Less', 'than', '1', 'year),', 'Database', 'Management', 'System', '(Less', 'than', '1', 'year),', 'Java', '(Less', 'than', '1', 'year)', 'ADDITIONAL', 'INFORMATION', 'Technical', 'Skills', 'https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN', '•', 'Programming', 'language:', 'C,', 'C++,', 'Java', '•', 'Oracle', 'PeopleSoft', '•', 'Internet', 'Of', 'Things', '•', 'Machine', 'Learning', '•', 'Database', 'Management', 'System', '•', 'Computer', 'Networks', '•', 'Operating', 'System', 'worked', 'on:', 'Linux,', 'Windows,', 'Mac', 'Non', '-', 'Technical', 'Skills', '•', 'Honest', 'and', 'Hard-Working', '•', 'Tolerant', 'and', 'Flexible', 'to', 'Different', 'Situations', '•', 'Polite', 'and', 'Calm', '•', 'Team-Player']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Email Address', 'I-Email Address', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Graduation Year', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Designation', 'I-Designation', 'I-Designation', 'I-Designation', 'I-Designation', 'I-Designation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Graduation Year', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 4: Convert Tokens and Labels",
   "id": "c2036130f54601a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:25.042552Z",
     "start_time": "2024-05-29T15:14:21.621218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 4: Convert Tokens and Labels\n",
    "# BERT requires inputs in the form of token IDs and attention masks. You need to convert your tokens and labels into these formats.\n",
    "\n",
    "# Define the maximum sequence length\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Tokenize and pad the tokens and labels\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "label_ids = []\n",
    "\n",
    "label_map = {\"O\": 0}  # Initialize the label map with 'O' as 0\n",
    "for label_list in labels:\n",
    "    for label in label_list:\n",
    "        if label not in label_map:\n",
    "            label_map[label] = len(label_map)\n",
    "\n",
    "for tokens, label_list in zip(tokenized_texts, labels):\n",
    "    # Tokenize the tokens\n",
    "    tokenized_input = tokenizer.encode_plus(\n",
    "        tokens,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    input_ids.append(tokenized_input['input_ids'])\n",
    "    attention_masks.append(tokenized_input['attention_mask'])\n",
    "    \n",
    "    # Convert labels to IDs and pad them\n",
    "    label_ids.append(\n",
    "        pad_sequences(\n",
    "            [[label_map[label] for label in label_list]],\n",
    "            maxlen=MAX_LEN,\n",
    "            value=label_map[\"O\"],\n",
    "            padding='post',\n",
    "            truncating='post'\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "input_ids = np.array(input_ids).squeeze()\n",
    "attention_masks = np.array(attention_masks).squeeze()\n",
    "label_ids = np.array(label_ids)\n",
    "\n",
    "# Check the shapes of the input IDs, attention masks, and label IDs\n",
    "print(\"Shape of input IDs: \", input_ids.shape)\n",
    "print(\"Attention marks: \", attention_masks.shape)\n",
    "print(\"label IDs: \", label_ids.shape)"
   ],
   "id": "a7bd4d20308de04f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input IDs:  (220, 128)\n",
      "Attention marks:  (220, 128)\n",
      "label IDs:  (220, 128)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Split the data into training & validation sets for training the NER model",
   "id": "60fd99161b7170ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:29.057366Z",
     "start_time": "2024-05-29T15:14:29.038088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split the data into training and validation sets\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    input_ids, label_ids, test_size=0.1, random_state=40\n",
    ")\n",
    "train_masks, val_masks, _, _ = train_test_split(\n",
    "    attention_masks, input_ids, test_size=0.1, random_state=40\n",
    ")"
   ],
   "id": "725aa7d71958d5f4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 5: Create TensorFlow Datasets",
   "id": "cf8e0c650a2b82e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:32.571230Z",
     "start_time": "2024-05-29T15:14:32.512323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Create TensorFlow Datasets\n",
    "# Convert the data into TensorFlow datasets.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_masks, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_masks, val_labels))\n",
    "\n",
    "def encode_examples(input_ids, attention_masks, label_ids):\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_masks}, label_ids\n",
    "\n",
    "train_dataset = train_dataset.map(encode_examples).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(encode_examples).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ],
   "id": "c0bd00b265868785",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8edfa2d7c798c786"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 6: Building the Model (bert-base-uncased)",
   "id": "40858adee84bd402"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:37.862435Z",
     "start_time": "2024-05-29T15:14:37.016932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Step 6: Define the NER Model\n",
    "# # Define the NER model using BERT as the base and a classification layer on top.\n",
    "# \n",
    "# # Define the model configuration\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "# \n",
    "# # Load the model: bert-base-uncased is used as the base model\n",
    "# # bert-base-uncased - It is a pre-trained BERT model that has been lowercased.\n",
    "# model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "# \n",
    "# # # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.00003)     # What is optimizer: https://keras.io/api/optimizers/\n",
    "# # SparseCategoricalCrossentropy is used as the loss function since the labels are integers\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]      # SparseCategoricalAccuracy is used as the evaluation metric\n",
    "# \n",
    "# model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Building the Model (bert-base-uncased)\n",
    "\n",
    "# Define the model configuration\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Load the model: bert-base-uncased is used as the base model\n",
    "# bert-base-uncased - It is a pre-trained BERT model that has been lowercased.\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00003)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy')]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ],
   "id": "a267fb009fe64f22",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\GBC_Lenovo23\\Applied_AI_SolnDev_T431\\AASD_Semester2\\DL1_AASD4010\\venv_DL1\\.venv2-process-resume-ner-bert\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "352a71b3ec912b8a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 7: Training the Model",
   "id": "355ff3f9d83b7138"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:14:55.589546Z",
     "start_time": "2024-05-29T15:14:55.573909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# Get details of the GPU:\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)"
   ],
   "id": "253e7de74842ca9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1b6bbf8810c9f7e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:15:44.460006Z",
     "start_time": "2024-05-29T15:15:01.013379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 7: Training the Model\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8  # Adjusted batch size to avoid GPU memory issues\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_masks, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_masks, val_labels))\n",
    "\n",
    "def encode_examples(input_ids, attention_masks, label_ids):\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_masks}, label_ids\n",
    "\n",
    "train_dataset = train_dataset.map(encode_examples).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.map(encode_examples).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Custom training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    for batch, (inputs, labels) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(inputs, training=True).logits\n",
    "            loss_value = loss(labels, logits)\n",
    "\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Batch {batch}, Loss: {loss_value.numpy()}')\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    for batch, (inputs, labels) in enumerate(val_dataset):\n",
    "        logits = model(inputs, training=False).logits\n",
    "        loss_value = loss(labels, logits)\n",
    "        val_loss += loss_value.numpy()\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "\n",
    "        # Cast predictions to int32 to match the labels type\n",
    "        predictions = tf.cast(predictions, tf.int32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(predictions == labels, tf.float32))\n",
    "        val_accuracy += accuracy.numpy()\n",
    "\n",
    "    val_loss /= len(val_dataset)\n",
    "    val_accuracy /= len(val_dataset)\n",
    "    print(f'Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy}')\n",
    "\n",
    "# Save the model - Create a directory named 'Saved_Model' and saves the model in it\n",
    "model.save('Saved_Model/ner_bert_model', save_format='tf')"
   ],
   "id": "baca76f05b4dbb71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Batch 0, Loss: 3.1612813472747803\n",
      "Validation Loss: 0.2765776713689168, Validation Accuracy: 0.9603949586550394\n",
      "Epoch 2/3\n",
      "Batch 0, Loss: 0.4001616835594177\n",
      "Validation Loss: 0.26989200711250305, Validation Accuracy: 0.9603949586550394\n",
      "Epoch 3/3\n",
      "Batch 0, Loss: 0.2469308227300644\n",
      "Validation Loss: 0.2740538716316223, Validation Accuracy: 0.9603949586550394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 417). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Saved_Model/ner_bert_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Saved_Model/ner_bert_model\\assets\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4446dbb24238caa6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 8: Evaluating the Model",
   "id": "4f001bb6bb94afe4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T15:15:47.864047Z",
     "start_time": "2024-05-29T15:15:44.460006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 8: Evaluating the Model\n",
    "# Evaluate the model on the validation dataset.\n",
    "\n",
    "# # Load the saved model\n",
    "# model = tf.keras.models.load_model('Saved_Model/ner_bert_model')\n",
    "# # Evaluate the model\n",
    "# results = model.evaluate(val_dataset)\n",
    "# # Print the validation loss and accuracy\n",
    "# print(\"Validation Loss: \", results[0])\n",
    "# print(\"Validation Accuracy: \", results[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Evaluating the Model\n",
    "# Redefine the model with the correct architecture and load the weights directly\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Redefine the model\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))\n",
    "\n",
    "# Load the saved weights\n",
    "model.load_weights('Saved_Model/ner_bert_model')\n",
    "\n",
    "# Custom evaluation function\n",
    "def evaluate_model(model, dataset):\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    for batch, (inputs, labels) in enumerate(dataset):\n",
    "        logits = model(inputs, training=False).logits\n",
    "        loss_value = loss(labels, logits)\n",
    "        val_loss += loss_value.numpy()\n",
    "        predictions = tf.argmax(logits, axis=-1)\n",
    "\n",
    "        # Cast predictions to int32 to match the labels type\n",
    "        predictions = tf.cast(predictions, tf.int32)\n",
    "        accuracy = tf.reduce_mean(tf.cast(predictions == labels, tf.float32))\n",
    "        val_accuracy += accuracy.numpy()\n",
    "\n",
    "    val_loss /= len(dataset)\n",
    "    val_accuracy /= len(dataset)\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = evaluate_model(model, val_dataset)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ],
   "id": "6acc57f713b2d6b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2740538716316223\n",
      "Validation Accuracy: 0.9603949586550394\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b911cb9a40c19dbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c836edc97e6d663d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c92ebb9a831cc0b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8d9687c2120133a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Loading the data from the data.json file\n",
    "# Step 2: Exploring the loaded data\n",
    "# Step 3: Pre-process the data\n",
    "# Step 4: Convert Tokens and Labels\n",
    "# Step 5: Split the data into training & validation sets for training the NER model\n",
    "# Step 5: Create TensorFlow Datasets\n",
    "\n",
    "# Step 6: Building the Model (bert-base-uncased)\n",
    "# Step 7: Training the Model\n",
    "# Step 8: Evaluating the Model\n"
   ],
   "id": "5afa6e2e912f7b65"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
